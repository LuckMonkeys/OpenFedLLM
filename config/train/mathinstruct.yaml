#model
model_name_or_path: "/home/zx/public/model-hub/huggingface/meta-llama/Llama-2-7b-hf"
cache_dir: null

#dataset
dataset_name: "TIGER-Lab/MathInstruct"
template: "alpaca"
seed: 2023
dpo_beta: 0.1
dataset_sample: 20000
local_data_dir: null
na_tasks_file: null

#training
log_with: "none"
learning_rate: 2e-5
batch_size: 4 # 16
seq_length: 512
gradient_accumulation_steps: 1
use_auth_token: false
num_train_epochs: 3
max_steps: 40 #10

#eval
eval_batch_size: 4 #16 
# eval_method: generate


#quantization
load_in_8bit: true
load_in_4bit: false

#peft tuning
use_peft: true
trust_remote_code: false
peft_lora_r: 8
peft_lora_alpha: 16
peft_target_modules: default

#log
output_dir: "output"
logging_steps: 100
save_steps: 1000
save_total_limit: 10
push_to_hub: false
hub_model_id: null
gradient_checkpointing: true

#tes
debug: false
